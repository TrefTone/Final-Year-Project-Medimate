{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction:"
      ],
      "metadata": {
        "id": "5KmOeHAhmJmR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Augmentation for NLP\n",
        "\n",
        "This notebook implements various data augmentation techniques for text with the goal of increasing the diversity of the training data. Data augmentation in NLP is a method to synthetically create more data to improve model generalization, without needing additional labeled examples.\n",
        "\n",
        "### Theory of Data Augmentation in NLP\n",
        "\n",
        "Data augmentation is widely used in Natural Language Processing (NLP) to improve model performance by introducing variations in the training data. By augmenting the text, the model is exposed to a wider range of input formats and becomes more robust. Augmentation can be especially useful in tasks like text classification, machine translation, or question-answering systems.\n",
        "\n",
        "### Techniques Used in This Notebook:\n",
        "\n",
        "1. ### Synonym Replacement\n",
        "   - **Description**: This technique replaces words in the text with their synonyms, aiming to maintain the meaning of the sentence while increasing lexical diversity.\n",
        "   - **Method**: For each word in the sentence, we search for synonyms using the WordNet lexical database. The first synonym is used as a replacement, and if no synonym is found, the word remains unchanged.\n",
        "   - **Example**:\n",
        "     - Original: \"The quick brown fox\"\n",
        "     - Synonym Replaced: \"The quick brown trick\"\n",
        "   \n",
        "2. ### Random Insertion\n",
        "   - **Description**: Random words or their synonyms are inserted at random positions within the sentence. This adds variation in sentence structure, which makes the model more robust to unexpected word orders.\n",
        "   - **Method**: A random word is chosen from the sentence, and one of its synonyms is inserted at a random position.\n",
        "   - **Example**:\n",
        "     - Original: \"The quick brown fox\"\n",
        "     - Random Insertion: \"The quick brown fox fox\"\n",
        "\n",
        "3. ### Random Deletion\n",
        "   - **Description**: Words in the sentence are randomly removed with a set probability `p`. This trains the model to handle missing words or incomplete data, simulating noisy input.\n",
        "   - **Method**: Each word has a `p` probability (default: 20%) of being removed. If all words are deleted, one word is retained to ensure the sentence is not empty.\n",
        "   - **Example**:\n",
        "     - Original: \"The quick brown fox\"\n",
        "     - Random Deletion: \"The quick fox\"\n",
        "\n",
        "4. ### Random Shuffling\n",
        "   - **Description**: The order of the words in the sentence is randomly shuffled, altering the sentence structure while keeping the same words. This helps the model learn to handle differently ordered inputs.\n",
        "   - **Method**: Words in the sentence are shuffled randomly.\n",
        "   - **Example**:\n",
        "     - Original: \"The quick brown fox\"\n",
        "     - Shuffled: \"brown fox quick The\"\n",
        "\n",
        "5. ### Noise Addition\n",
        "   - **Description**: This technique adds random keyboard noise, simulating typographical errors made by humans. This helps train models that deal with human input, such as auto-correction systems.\n",
        "   - **Method**: Using the `nlpaug` library's `KeyboardAug` augmenter, random keyboard errors are introduced into the text (e.g., mistyping 'e' as 'r').\n",
        "   - **Example**:\n",
        "     - Original: \"The quick brown fox\"\n",
        "     - Noise Added: \"The q7uick brown fox\"\n",
        "\n",
        "### Application of Augmentations\n",
        "\n",
        "In the following code, for each phrase in the dataset, we apply the following augmentations:\n",
        "- Synonym Replacement\n",
        "- Random Insertion\n",
        "- Random Deletion\n",
        "- Random Shuffling\n",
        "- Noise Addition\n",
        "\n",
        "These augmentations increase the variety of the input data from a single sentence into multiple versions, making the model more robust and less prone to overfitting.\n",
        "\n",
        "### Benefits of Augmentation:\n",
        "- **Generalization**: The model is exposed to diverse examples, which helps it generalize better to unseen data.\n",
        "- **Error Resilience**: Augmentations like noise addition and random deletion simulate real-world data noise, making the model more resilient to input errors.\n",
        "- **Lexical Diversity**: Synonym replacement and random insertion increase lexical diversity, improving the model's ability to handle paraphrased inputs.\n"
      ],
      "metadata": {
        "id": "Ieul0h2j_BM3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code:"
      ],
      "metadata": {
        "id": "Tj3ZlZqrmGlR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nlpaug"
      ],
      "metadata": {
        "id": "ucx3EtE-mU57",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "698c1251-717f-4ec4-a31e-055a311a0efe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting googletrans\n",
            "  Downloading googletrans-3.0.0.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting httpx==0.13.3 (from googletrans)\n",
            "  Downloading httpx-0.13.3-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans) (2024.8.30)\n",
            "Collecting hstspreload (from httpx==0.13.3->googletrans)\n",
            "  Downloading hstspreload-2024.10.1-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans) (1.3.1)\n",
            "Collecting chardet==3.* (from httpx==0.13.3->googletrans)\n",
            "  Downloading chardet-3.0.4-py2.py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting idna==2.* (from httpx==0.13.3->googletrans)\n",
            "  Downloading idna-2.10-py2.py3-none-any.whl.metadata (9.1 kB)\n",
            "Collecting rfc3986<2,>=1.3 (from httpx==0.13.3->googletrans)\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting httpcore==0.9.* (from httpx==0.13.3->googletrans)\n",
            "  Downloading httpcore-0.9.1-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting h11<0.10,>=0.8 (from httpcore==0.9.*->httpx==0.13.3->googletrans)\n",
            "  Downloading h11-0.9.0-py2.py3-none-any.whl.metadata (8.1 kB)\n",
            "Collecting h2==3.* (from httpcore==0.9.*->httpx==0.13.3->googletrans)\n",
            "  Downloading h2-3.2.0-py2.py3-none-any.whl.metadata (32 kB)\n",
            "Collecting hyperframe<6,>=5.2.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans)\n",
            "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting hpack<4,>=3.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans)\n",
            "  Downloading hpack-3.0.0-py2.py3-none-any.whl.metadata (7.0 kB)\n",
            "Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Downloading hstspreload-2024.10.1-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m61.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
            "Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
            "Building wheels for collected packages: googletrans\n",
            "  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletrans: filename=googletrans-3.0.0-py3-none-any.whl size=15719 sha256=f11d20fb64cc6abaa89bf25a01821de63b959cba2d8e600cc67de43bb7faf2d2\n",
            "  Stored in directory: /root/.cache/pip/wheels/b3/81/ea/8b030407f8ebfc2f857814e086bb22ca2d4fea1a7be63652ab\n",
            "Successfully built googletrans\n",
            "Installing collected packages: rfc3986, hyperframe, hpack, h11, chardet, idna, hstspreload, h2, httpcore, httpx, googletrans\n",
            "  Attempting uninstall: chardet\n",
            "    Found existing installation: chardet 5.2.0\n",
            "    Uninstalling chardet-5.2.0:\n",
            "      Successfully uninstalled chardet-5.2.0\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.10\n",
            "    Uninstalling idna-3.10:\n",
            "      Successfully uninstalled idna-3.10\n",
            "Successfully installed chardet-3.0.4 googletrans-3.0.0 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2024.10.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 idna-2.10 rfc3986-1.5.0\n",
            "Collecting nlpaug\n",
            "  Downloading nlpaug-1.1.11-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: numpy>=1.16.2 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (1.26.4)\n",
            "Requirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (2.32.3)\n",
            "Requirement already satisfied: gdown>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (5.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (4.12.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (3.16.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (4.66.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.0->nlpaug) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.0->nlpaug) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.0->nlpaug) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (2.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (2024.8.30)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.2.0->nlpaug) (1.16.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug) (2.6)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown>=4.0.0->nlpaug) (1.7.1)\n",
            "Downloading nlpaug-1.1.11-py3-none-any.whl (410 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.5/410.5 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nlpaug\n",
            "Successfully installed nlpaug-1.1.11\n",
            "Collecting translate\n",
            "  Downloading translate-3.6.1-py2.py3-none-any.whl.metadata (7.7 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from translate) (8.1.7)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from translate) (4.9.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from translate) (2.32.3)\n",
            "Collecting libretranslatepy==2.1.1 (from translate)\n",
            "  Downloading libretranslatepy-2.1.1-py3-none-any.whl.metadata (233 bytes)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->translate) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->translate) (2.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->translate) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->translate) (2024.8.30)\n",
            "Downloading translate-3.6.1-py2.py3-none-any.whl (12 kB)\n",
            "Downloading libretranslatepy-2.1.1-py3-none-any.whl (3.2 kB)\n",
            "Installing collected packages: libretranslatepy, translate\n",
            "Successfully installed libretranslatepy-2.1.1 translate-3.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "import nlpaug.augmenter.char as nac\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "CSeOTMzNmPqG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download WordNet for synonym replacement\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R0J29Rdhmlf2",
        "outputId": "914fe083-f1a8-4732-aaac-2e0a30caf14e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('')\n",
        "df.head()\n",
        "\n",
        "df = df[[\"phrase\",\"prompt\"]]"
      ],
      "metadata": {
        "id": "yyQ7nz7EmnpY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a noise augmenter for noise addition\n",
        "aug = nac.KeyboardAug()"
      ],
      "metadata": {
        "id": "wsD9Q2FSmycs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xn8kTrawkdq-"
      },
      "outputs": [],
      "source": [
        "# Function for synonym replacement\n",
        "def synonym_replacement(text):\n",
        "    words = text.split()\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        synonyms = wordnet.synsets(word)\n",
        "        if synonyms:\n",
        "            synonym = synonyms[0].lemmas()[0].name()\n",
        "            new_words.append(synonym)\n",
        "        else:\n",
        "            new_words.append(word)\n",
        "    return ' '.join(new_words)\n",
        "\n",
        "# Function for random insertion\n",
        "def random_insertion(text, n=1):\n",
        "    words = text.split()\n",
        "    for _ in range(n):\n",
        "        synonym = None\n",
        "        while synonym is None:\n",
        "            word = random.choice(words)\n",
        "            synonyms = wordnet.synsets(word)\n",
        "            if synonyms:\n",
        "                synonym = synonyms[0].lemmas()[0].name()\n",
        "        insert_pos = random.randint(0, len(words))\n",
        "        words.insert(insert_pos, synonym)\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Function for random deletion\n",
        "def random_deletion(text, p=0.2):\n",
        "    words = text.split()\n",
        "    if len(words) == 1:\n",
        "        return text\n",
        "\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        if random.uniform(0, 1) > p:\n",
        "            new_words.append(word)\n",
        "    return ' '.join(new_words) if new_words else random.choice(words)\n",
        "\n",
        "# # Function for shuffling\n",
        "def random_shuffling(text):\n",
        "    words = text.split()\n",
        "    random.shuffle(words)\n",
        "    return ' '.join(words)\n",
        "\n",
        "# # Function for noise addition\n",
        "def noise_addition(text):\n",
        "    return aug.augment(text)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply augmentations to the dataset\n",
        "augmented_data = []\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "    phrase = row['phrase']\n",
        "    prompt = row['prompt']\n",
        "\n",
        "    # Synonym Replacement\n",
        "    synonym_replaced = synonym_replacement(phrase)\n",
        "\n",
        "    # Random Insertion\n",
        "    random_inserted = random_insertion(phrase)\n",
        "\n",
        "    # Random Deletion\n",
        "    random_deleted = random_deletion(phrase)\n",
        "\n",
        "    # # Shuffling\n",
        "    shuffled = random_shuffling(phrase)\n",
        "\n",
        "    # # Noise Addition\n",
        "    noisy = noise_addition(phrase)\n",
        "\n",
        "    # Add the augmented data to the list\n",
        "    augmented_data.append([synonym_replaced, prompt])\n",
        "    augmented_data.append([random_inserted, prompt])\n",
        "    augmented_data.append([random_deleted, prompt])\n",
        "    augmented_data.append([shuffled, prompt])\n",
        "    augmented_data.append([noisy, prompt])"
      ],
      "metadata": {
        "id": "XfiEauzjm3cX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert augmented data to a DataFrame\n",
        "augmented_df = pd.DataFrame(augmented_data, columns=['phrase', 'prompt'])\n",
        "\n",
        "# Combine with the original data if desired\n",
        "final_df = pd.concat([df, augmented_df])\n",
        "\n",
        "# Save the augmented dataset to a new CSV file\n",
        "final_df.to_csv('augmented_dataset.csv', index=False)\n",
        "\n",
        "# Combine original and augmented data into a DataFrame\n",
        "final_df = pd.concat([df, augmented_df])\n",
        "\n",
        "# Shuffle the DataFrame\n",
        "final_df = final_df.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "# Save the shuffled dataset to a new CSV file\n",
        "final_df.to_csv('shuffled_augmented_dataset.csv', index=False)\n",
        "\n",
        "print(\"Data augmentation completed and saved to 'augmented_dataset.csv'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DPLIqFuum6Np",
        "outputId": "08f0e518-6b13-40d8-e408-25e4125389fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data augmentation completed and saved to 'augmented_dataset.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "07v4fqZQqKKQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4er5wDvRwur9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}